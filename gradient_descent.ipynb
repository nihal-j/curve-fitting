{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDf = pd.read_csv('data.csv')\n",
    "x = np.array(dataDf['LATITUDE'].tolist())\n",
    "y = np.array(dataDf['LONGITUDE'].tolist())\n",
    "T = np.array(dataDf['ALTITUDE'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(vec, type = 'standardization'):\n",
    "    \n",
    "    '''\n",
    "    Performs normalization on the vector. The type of normalization can be standardization or\n",
    "    min-max normalization.\n",
    "    \n",
    "    Agruments:\n",
    "        vec: the vector that has to be normalized\n",
    "        type: 'standardization' or 'min-max'. Default is standardization\n",
    "        \n",
    "    Returns:\n",
    "        vec: the normalized vector\n",
    "    '''\n",
    "    \n",
    "    if type == 'standardization':\n",
    "        return (vec - np.mean(vec)) / np.std(vec)\n",
    "    \n",
    "    if type == 'min-max':\n",
    "        return (vec - min(vec)) / (max(vec) - min(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x, y, T):\n",
    "    \n",
    "    '''\n",
    "    Splits the vectors into training, cross-validation and test sets,\n",
    "    following 80%, 10% and 10% sizes respectively.\n",
    "    \n",
    "    Arguments:\n",
    "        x: vector x of data (latitude)\n",
    "        y: vector y of data (longitude)\n",
    "        T: target variable vector of data (altitude)\n",
    "        \n",
    "    Returns:\n",
    "        x_train, y_train, T_train, x_val, y_val, T_val, x_test, y_test, T_test\n",
    "        following the description above\n",
    "    '''\n",
    "    \n",
    "    X = [(x[i], y[i]) for i in range(len(x))]\n",
    "    X_train, X_test, T_train, T_test = train_test_split(X, T, test_size = 0.2, random_state = 0)\n",
    "    X_test, X_val, T_test, T_val = train_test_split(X_test, T_test, test_size = 0.5, random_state = 42)\n",
    "    \n",
    "    x_test = np.array([X_test[i][0] for i in range(len(X_test))])\n",
    "    y_test = np.array([X_test[i][1] for i in range(len(X_test))])\n",
    "    \n",
    "    x_val = np.array([X_val[i][0] for i in range(len(X_val))])\n",
    "    y_val = np.array([X_val[i][1] for i in range(len(X_val))])\n",
    "    \n",
    "    x_train = np.array([X_train[i][0] for i in range(len(X_train))])\n",
    "    y_train = np.array([X_train[i][1] for i in range(len(X_train))])\n",
    "                      \n",
    "    return x_train, y_train, T_train, x_val, y_val, T_val, x_test, y_test, T_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(x, y, N, deg):\n",
    "    \n",
    "    '''\n",
    "    Generates the feature matrix for each pow of y (dy), get all pow of x (dx), \n",
    "    such that dx + dy = deg\n",
    "    \n",
    "    The feature matrix looks as:\n",
    "    \n",
    "    [[1, x1, x1^2, y1, x1y1, y1^2]\n",
    "     [1, x2, x2^2, y2, x2y2, y2^2]\n",
    "      .  .\n",
    "      .  .\n",
    "     [1, xN, xN^2, yN, xNyN, yN^2]]\n",
    "     \n",
    "     Arguments:\n",
    "        x: vector x of data (latitude)\n",
    "        y: vector y of data (longitude)\n",
    "        N: no. of training examples\n",
    "        deg: maximum degree upto which features need to be calculated\n",
    "        \n",
    "    Returns:\n",
    "        featureMatrix: the featureMatrix as described\n",
    "        d: no. of features (no. of columns in featureMatrix)\n",
    "     '''\n",
    "    \n",
    "    featureMatrix = []\n",
    "    \n",
    "    # number of features\n",
    "    d = 0\n",
    "    \n",
    "    if N == 1:\n",
    "        x = [x]\n",
    "        y = [y]\n",
    "    \n",
    "    for n in range(N):\n",
    "        row = []\n",
    "        for i in range(deg + 1):\n",
    "            for j in range(deg - i + 1):\n",
    "                term = (x[n]**j) * (y[n]**i)\n",
    "                row.append(term)\n",
    "                if n == 0:\n",
    "                    d += 1\n",
    "        featureMatrix.append(row)\n",
    "    \n",
    "    # converting to a numpy array\n",
    "    featureMatrix = np.array(featureMatrix)\n",
    "    \n",
    "    return featureMatrix, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(W, features, error, eta, lamb = 0, reg = 'L2'):\n",
    "    \n",
    "    '''\n",
    "    Updates weights using regularization (if lamb != 0), following\n",
    "    'L1' or 'L2' regularization. Default is 'L2' regularization.\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    delta = features.dot(error)\n",
    "    if reg == 'L2':\n",
    "        delta += lamb*W\n",
    "    else:\n",
    "        Wdel = np.copy(W)\n",
    "        for i in range(len(Wdel)):\n",
    "            if Wdel[i] > 0:\n",
    "                Wdel[i] = 1\n",
    "            if Wdel[i] < 0:\n",
    "                Wdel[i] = -1\n",
    "            else:\n",
    "                Wdel[i] = 0\n",
    "        delta += lamb*Wdel\n",
    "    \n",
    "    W = W - eta*delta\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(W, error, d):\n",
    "    \n",
    "    for i in range(d):\n",
    "        print(W[i])\n",
    "    print('Error: ' + str(error) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W, X):\n",
    "    \n",
    "    return X.dot(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(x, y, T, deg, maxIter, eta, lamb = 0, reg = 'L2'):\n",
    "       \n",
    "    N = len(x)\n",
    "    featureMatrix, d = generate_features(x, y, N, deg)\n",
    "    \n",
    "    # initial weights vector\n",
    "    # random initialization\n",
    "    random.seed(12)\n",
    "    W = np.array([random.random() for i in range(d)])\n",
    "    # zero initialization\n",
    "    # W = np.array([0 for i in range(d)])\n",
    "    \n",
    "    prevError = 0\n",
    "    currentError = 0\n",
    "    errors = []\n",
    "    \n",
    "    for i in tqdm_notebook(range(maxIter)):\n",
    "        \n",
    "        H = predict(W, featureMatrix)\n",
    "        if reg == 'L2':\n",
    "            E = (0.5*(H - T).dot(np.transpose(H - T))) + (0.5*lamb*sum(W*W))\n",
    "        else:\n",
    "            E = (0.5*(H - T).dot(np.transpose(H - T))) + (0.5*lamb*sum((W*W)**0.5))\n",
    "        prevError = currentError\n",
    "        currentError = E\n",
    "        errors.append(currentError)\n",
    "        #=======\n",
    "        # Perform termination check here using prevError and currentError\n",
    "        #=======\n",
    "        # print(0.5*lamb*sum(W*W))\n",
    "        # show(W, currentError, d)\n",
    "        W = update_weights(W, np.transpose(featureMatrix), H - T, eta, lamb, reg)\n",
    "        \n",
    "    return W, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stochastic(x, y, T, deg, maxIter, eta, lamb = 0):\n",
    "    \n",
    "    random.seed(12)\n",
    "    featureMatrix, d = generate_features(x[0], y[0], 1, deg)\n",
    "    W = np.array([random.random() for i in range(d)])\n",
    "    errors = []\n",
    "    N = len(T)\n",
    "    \n",
    "    for i in range(maxIter):\n",
    "        \n",
    "        featureMatrix, d = generate_features(x[i%N], y[i%N], 1, deg)\n",
    "        H = predict(W, featureMatrix)\n",
    "        E = (0.5*(H - T[i%N]).dot(np.transpose(H - T[i%N]))) + (0.5*lamb*sum(W*W))\n",
    "        errors.append(E)\n",
    "        W = update_weights(W, np.transpose(featureMatrix), H - T[i%N], eta, lamb)\n",
    "        \n",
    "    return W, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_R2(T, H):\n",
    "    \n",
    "    tss = sum((T - np.mean(T))*(T - np.mean(T)))\n",
    "    rss = sum((T - H)*(T - H))\n",
    "    \n",
    "    return 1 - (rss/tss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rmse(T, H):\n",
    "    \n",
    "    se = sum((T - H)*(T - H))\n",
    "    mse = se/len(T)\n",
    "    rmse = mse ** 0.5\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(errors, numIters, interval):\n",
    "    \n",
    "    xLabels = [i for i in range(0, numIters) if i % interval == 0]\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(xLabels, [errors[i] for i in range(len(errors)) if i % interval == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(x, y, T, deg):\n",
    "    \n",
    "    N = len(x)\n",
    "    featureMatrix, d = generate_features(x, y, N, deg)\n",
    "    \n",
    "    W = np.matmul(np.transpose(featureMatrix), featureMatrix)\n",
    "    W = np.linalg.inv(W)\n",
    "    W = np.matmul(W, np.transpose(featureMatrix))\n",
    "    W = np.dot(W, T)\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_lambda(x, y, T, reg = 'L2'):\n",
    "    \n",
    "    x_train, y_train, T_train, x_val, y_val, T_val, x_test, y_test, T_test = split(x, y, T)\n",
    "    \n",
    "    W = np.array([])\n",
    "    curr_lambda = 0\n",
    "    \n",
    "    l = 0\n",
    "    r = 1000\n",
    "    numIters = 1000\n",
    "    cnt = 0\n",
    "    \n",
    "    lambdas = []\n",
    "    rmses = []\n",
    "    \n",
    "    while l <= r and cnt < 35:\n",
    "        \n",
    "        curr_lambda = (l + r) / 2\n",
    "        W, errors = generate_model(x_train, y_train, T_train, 1, numIters, 0.0000003, curr_lambda, reg)\n",
    "        X, num = generate_features(x_val, y_val, len(x_val), 1)\n",
    "        H = predict(W, X)\n",
    "        rmse = calc_rmse(T_val, H)\n",
    "        lambdas.append(curr_lambda)\n",
    "        rmses.append(rmse)\n",
    "        \n",
    "        W1, errors = generate_model(x_train, y_train, T_train, 1, numIters, 0.0000003, curr_lambda+0.01*curr_lambda, reg)\n",
    "        X, num = generate_features(x_val, y_val, len(x_val), 1)\n",
    "        H = predict(W1, X)\n",
    "        rmse1 = calc_rmse(T_val, H)\n",
    "        W2, errors = generate_model(x_train, y_train, T_train, 1, numIters, 0.0000003, curr_lambda-0.01*curr_lambda, reg)\n",
    "        X, num = generate_features(x_val, y_val, len(x_val), 1)\n",
    "        H = predict(W2, X)\n",
    "        rmse2 = calc_rmse(T_val, H)\n",
    "        \n",
    "        if rmse < rmse1 and rmse < rmse2:\n",
    "            break\n",
    "            \n",
    "        elif rmse < rmse1 and rmse > rmse2:\n",
    "            r = curr_lambda\n",
    "            \n",
    "        elif rmse > rmse1 and rmse < rmse2:\n",
    "            l = curr_lambda\n",
    "            \n",
    "        cnt += 1\n",
    "        print(cnt)\n",
    "        \n",
    "    X, num = generate_features(x_test, y_test, len(x_test), 1)\n",
    "    H = predict(W, X)\n",
    "    print(W)\n",
    "    print('lambda = ', curr_lambda)\n",
    "    print('R2 error: ', calc_R2(T_test, H))    \n",
    "    print('RMS error: ', calc_rmse(T_test, H))\n",
    "    \n",
    "    return W, curr_lambda, lambdas, rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scikit_regression(x, y, T):\n",
    "    \n",
    "    data = [[x[i],y[i]] for i in range(len(x))]\n",
    "    reg = linear_model.LinearRegression()\n",
    "    reg.fit(data, T)\n",
    "    return np.concatenate((np.array([reg.intercept_]),reg.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizedx = normalize(x)\n",
    "normalizedy = normalize(y)\n",
    "normalizedT = normalize(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For degree  1\n",
      "R2 error:  0.02604788130660507\n",
      "RMS error:  18.40506370109135\n",
      "[22.15346877 -3.54770279  2.75203942]\n",
      "[22.15346877 -3.54770279  2.75203942]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# x_train, y_train, T_train, x_val, y_val, T_val, x_test, y_test, T_test = split(x, y, T)\n",
    "x_train, y_train, T_train, x_val, y_val, T_val, x_test, y_test, T_test = split(normalizedx, normalizedy, T)\n",
    "# x_train, y_train, T_train = normalizedx, normalizedy, normalizedT\n",
    "# x_train, y_train, T_train = x, y, T\n",
    "\n",
    "for i in range(1,2):\n",
    "    print('For degree ', i)\n",
    "    numIters = 1000\n",
    "    W = normal_equation(x_train, y_train, T_train, 1)\n",
    "    # W, errors = generate_model(x_train, y_train, T_train, i, numIters, 0.0000003, 1, 'L1')\n",
    "    # W, errors = generate_stochastic(x_train, y_train, T_train, 1, len(x_train)*2, 0.03)\n",
    "    # print('Final loss: ', errors[len(errors) - 1])\n",
    "    # best_model, best_lambda, lambdas, rmses = get_best_lambda(normalizedx, normalizedy, T, 'L2')\n",
    "    # plt.scatter(lambdas, rmses)\n",
    "    # plot_loss(errors, numIters, 20)\n",
    "    # plt.savefig('Part_B.png')\n",
    "    X, num = generate_features(x_val, y_val, len(x_val), i)\n",
    "    H = predict(W, X)\n",
    "    print('R2 error: ', calc_R2(T_val, H))    \n",
    "    print('RMS error: ', calc_rmse(T_val, H))\n",
    "    print(W)\n",
    "    print(scikit_regression(x_train, y_train, T_train))\n",
    "    print('\\n\\n')\n",
    "\n",
    "# best_model, best_lambda, lambdas, rmses = get_best_lambda(normalizedx, normalizedy, T)\n",
    "# plt.scatter(lambdas, rmses)\n",
    "# plt.plot(lambdas, rmses)\n",
    "\n",
    "# print(scikit_regression(x_train, y_train, T_train))\n",
    "# W, errors = generate_stochastic(x_train, y_train, T_train, 1, len(x_train)//2, 0.03)\n",
    "# print(W)\n",
    "# W, errors = generate_stochastic(x_train, y_train, T_train, 1, len(x_train), 0.03)\n",
    "# print(W)\n",
    "# W, errors = generate_stochastic(x_train, y_train, T_train, 1, len(x_train)*2, 0.03)\n",
    "# print(W)\n",
    "# W, errors = generate_stochastic(x_train, y_train, T_train, 1, len(x_train)*4, 0.03)\n",
    "# print(W)\n",
    "\n",
    "# plt.xlabel('Reg. parameter (lambda)')\n",
    "# plt.ylabel('Root Mean Square Error')\n",
    "# plt.ylim([min(rmses) - 0.00000000001*min(rmses), max(rmses)+ 0.00000000001*max(rmses)])\n",
    "# plt.savefig('Part_C_L2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unnormalized\n",
    "\n",
    "## Degree = 2\n",
    "### Max iterations = 10,000\n",
    "### eta = 0.0000000000001\n",
    "\n",
    "0.4744234599520871      1 <br>\n",
    "0.6474382918705535      x <br>\n",
    "0.0005804929584987999   x^2 <br>\n",
    "0.14105769027351664     y <br>\n",
    "-0.09403124677459616    xy <br>\n",
    "0.3544625440157649      y^2 <br>\n",
    "\n",
    "Error:  75436974.66365339\n",
    "\n",
    "---------------------------------------------------\n",
    "\n",
    "## Degree = 1\n",
    "### Max iterations = 10,000\n",
    "### eta = 0.000000001\n",
    "\n",
    "0.5185508389987425       1 <br>\n",
    "0.15099146093306134      x <br>\n",
    "1.3394518499352581       y <br>\n",
    "\n",
    "Error:  75282294.06572284\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized\n",
    "\n",
    "## Degree = 2\n",
    "### Max iterations = 1,000\n",
    "### eta = 0.000002\n",
    "\n",
    "0.15263109107088374 1 <br>\n",
    "-0.046585920675156704 x <br>\n",
    "0.21581304287314088 x^2 <br>\n",
    "0.3174109858230444 y <br>\n",
    "-0.39821638060691267 xy <br>\n",
    "-0.1120204638382443 y^2 <br>\n",
    "\n",
    "Error: 3492.6023185812373\n",
    "\n",
    "---------------------------------------------------\n",
    "\n",
    "## Degree = 1\n",
    "### Max iterations = 1,000\n",
    "### eta = 0.000002\n",
    "\n",
    "0.20875515071358333      1 <br>\n",
    "-0.10010713951478299     x <br>\n",
    "0.09525453491739086      y <br>\n",
    "\n",
    "Error: 3585.734618153786"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
